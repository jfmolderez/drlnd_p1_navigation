{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithm ###\n",
    "The DQN(Deep QNetwork) algorithm approximates an optimal Q value function $q_*$ mapping state action pairs to their values. This function is represented as a Neural Network whose weights $w$ are learnt as the agent interacts with its environment. Each interaction is represented as a tuple\n",
    "$$<s, a, r, s'>$$\n",
    "-  $s$ is the current state observed from the environment\n",
    "-  $a$ is the action chosen by the agent based on an $\\epsilon$-greedy policy\n",
    "-  $r$ is the reward collected by following action $a$ in state $s$\n",
    "-  $s'$ is the next state reached by by following action $a$ in state $s$\n",
    "\n",
    "The successive interactions are stored in a replay buffer. The agent samples periodically a minibatch of interactions from the buffer. The use of this replay buffer prevents correlations between successive interactions. The agent uses the sampled interactions to compute a loss function and to update the weights of its networks using gradient descent. To this end, the agent is equipped with a double Q-Value network (DDQN) composed of a local network (the main one whose weights are updated using gradient descent) and a target network (a mirror of the local network that is updated more slowly) in charge of providing stable targets. The weights of the local network are updated as follows : \n",
    "$$\\Delta w = \\alpha * [R + \\gamma * max_{a} \\hat{q}(s', a, w^{-}) - \\hat{q}(s, a, w)] * \\nabla \\hat{q}(s, a, w)$$\n",
    "where :\n",
    "- $\\hat{q}(s, a, w)$ is the estimate of the qvalue for the current state $s$ and chosen action $a$ provided by the local network (weights $w$)\n",
    "- $\\hat{q}(s', a, w^{-})$ is the estimate of the qvalue for the next state $s'$ and an action $a$ provided by the target network (weights $w^{-}$)\n",
    "- $R + \\gamma * max_{a} \\hat{q}(s', a, w^{-}) - \\hat{q}(s, a, w)$ is the loss or temporal difference (TD) between the current estimate and the target which is the sum of the current reward and the discounted qvalue of the next state. \n",
    "\n",
    "The weights $w^{-}$ of the target network follow the updates of the $w$ more slowly (soft updates):\n",
    "    $$w^{-} = \\tau * w + (1 - \\tau) * w^{-}$$\n",
    "Without these more stable targets, we would encounter a harmful form of correlation whereby we shift the weights of the network based on constantly moving targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "- BUFFER_SIZE : the capacity of the replay buffer - a store of samples $<s, a, r, s'>$\n",
    "- BATCH_SIZE : the minibatch size that is sampled from the replay buffer and used to train the local network\n",
    "- GAMMA : factor used to discount the future rewards\n",
    "- TAU : factor used for the soft updates of the weights in the target network\n",
    "- LR : learning rate\n",
    "- UPDATE_EVERY : periodicity in steps : how often the weights are updated\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "````\n",
    "- n_episodes (2000) : number of episodes required to train an agent where each episode is a sequence of interactions between the agent and its environment. An episode has a maximal length or can be ended by the environment (flag done)\n",
    "- max_t (1000) : maximal length of an episode\n",
    "- epsilon : parameter for $\\epsilon$-greedy policy. With probability $\\epsilon$, the agent chooses an action at random (exploration) or with probability 1 - $\\epsilon$ chooses the best estimate for action (exploitation).The actual value of $\\epsilon$ decreases exponentially (see following parameters) to allow full exploration when learning starts and minimal exploration when learning ends.\n",
    "- eps_start (1.0) : full exploration, value of $\\epsilon$ when learning starts\n",
    "- eps_end (0.01) : minimal exploration, value of $\\epsilon$ after a sufficient number of leraning interactions\n",
    "- eps_decay (0.995) : rate of exponential decay for $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Rewards\n",
    "The following plot shows the evolution of the scores during the training phase. A score average of 15 over the last 100 episodes has been considered enough to end the learning phase. More specifically, the environment has been solved in 649 episodes with an average score of 15.04 over the last 100 episodes.  \n",
    "![](scores.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "- Tuning of hyperparamaters : in particular the learning rate, the minibatch size and epsilon\n",
    "- Prioritized sampling : priorities are attached to the interactions that reflect their temporal differences and the interactions are sampled according to these priorities (i.e. non uniformly) but this requires also an adaptation to the loss function to prevent bias that could originate from a non uniform distribution.\n",
    "- Dueling networks: implement a network producing two streams, the value of the state and the advantage of an action where the resulting q-value is a combination of the state value and of the advantage of the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
